{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2adcd1-ca23-4452-a439-852c79622b45",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a66a3c-7e5d-4ec5-baf4-af5a2c3cd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "from itertools import groupby\n",
    "import random\n",
    "import numba\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Gumbel\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "#Import necessary functions\n",
    "sys.path.append(\"aux_functions/\")\n",
    "from functions_datasets_global import global_model as DataBase # define what you import as DataBase\n",
    "from functions_datasets_global import validate_samples, manual_train_test_split\n",
    "from functions_loss_global import gaussian_distribution, nll_loss\n",
    "from functions_aux_global import create_folder, set_random_seed, write_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae359b-5db5-4698-91be-8189a7de425c",
   "metadata": {},
   "source": [
    "Data paths, parameters, and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44dc4bd4-25c7-42bc-a532-6f10647be29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'results_daymet_final' already exists.\n"
     ]
    }
   ],
   "source": [
    "path_entities_ids = 'path/loation_id/*.txt' # Text file for all location ids\n",
    "\n",
    "path_flagged = 'path/location_id/*.csv' # Path for flagged location ids (through quality control)\n",
    "\n",
    "path_data = 'path/data/timeseries/' # Path for separate dynamic timeseries\n",
    "path_static = 'path/static_attributes/*.csv' # Path for static attributes (single file)\n",
    "\n",
    "dynamic_input = ['precip_daymet', 'shortwave_rad_daymet', 't_max_daymet', 't_min_daymet', 'vapor_pressure_daymet']\n",
    "\n",
    "target = ['soil_moisture']\n",
    "\n",
    "static_input = ['lat', 'lon', 'depth', 'elevation', 'slope_degree', 'clay_fraction', 'silt_fraction', 'sand_fraction',\n",
    "               'climate_zones_5.0', 'climate_zones_6.0', 'climate_zones_7.0', 'climate_zones_8.0', 'climate_zones_9.0',\n",
    "                'climate_zones_10.0', 'climate_zones_11.0', 'climate_zones_12.0', 'climate_zones_13.0', 'climate_zones_14.0',\n",
    "                'climate_zones_15.0', 'climate_zones_16.0', 'climate_zones_18.0', 'land_cover_2.0', 'land_cover_4.0', 'land_cover_6.0',\n",
    "                'land_cover_9.0', 'land_cover_11.0', 'land_cover_12.0', 'land_cover_13.0', 'land_cover_14.0', 'land_cover_15.0', 'land_cover_16.0',\n",
    "                'land_cover_18.0', 'land_cover_20.0', 'land_cover_21.0', 'land_cover_22.0']\n",
    "\n",
    "model_hyper_parameters = {\n",
    "    \"input_size\": len(dynamic_input) + len(static_input),\n",
    "    \"no_of_layers\":2,  \n",
    "    \"seq_length\": 180,\n",
    "    \"hidden_size\": 128, \n",
    "    \"batch_size\": 64,\n",
    "    \"no_of_epochs\": 10,            \n",
    "    \"drop_out\": 0.3, \n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"adapt_learning_rate_epoch\": 1,\n",
    "    \"adapt_gamma_learning_rate\": 0.5,\n",
    "    \"set_forget_gate\":3\n",
    "}\n",
    "\n",
    "running_device = 'cpu'\n",
    "seed = 42\n",
    "\n",
    "with open(path_flagged, 'r') as f:\n",
    "    flagged_ids = set(\n",
    "        line.strip().replace('_filtered', '') for line in f if line.strip()\n",
    "    )\n",
    "    \n",
    "with open(path_entities_ids, \"r\") as f:\n",
    "    all_entity_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Exclude flagged locations\n",
    "all_entity_ids = [loc for loc in all_entity_ids if loc not in flagged_ids]\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.shuffle(all_entity_ids)\n",
    "\n",
    "path_save_folder = 'path/output_folder'\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10156d91-607b-4c05-8f26-85a8a0a9e034",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e75b99-c71c-482f-9c3a-99e54c808ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dynamic_input: List[str],\n",
    "                 static_input: List[str],\n",
    "                 set_type: str,\n",
    "                 target: List[str],\n",
    "                 sequence_length: int,\n",
    "                 path_entities_ids: str,\n",
    "                 path_data: str,\n",
    "                 path_static_attributes: str,\n",
    "                 check_NaN: bool = True):\n",
    "        \n",
    "        self.dynamic_input = dynamic_input\n",
    "        self.static_input = static_input\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.path_data = path_data\n",
    "        self.check_NaN = check_NaN\n",
    "\n",
    "        # Load train/test IDs\n",
    "        if isinstance(path_entities_ids, list):\n",
    "            self.entities_ids = path_entities_ids\n",
    "        else:\n",
    "            self.entities_ids = self._load_entity_ids(path_entities_ids)\n",
    "\n",
    "\n",
    "        # Load and filter static attributes\n",
    "        self.df_attributes = self._load_static_attributes(path_static_attributes)\n",
    "\n",
    "        # Initialize containers\n",
    "        self.sequence_data: Dict[str, Dict[str, torch.Tensor]] = {}\n",
    "        self.valid_samples = []\n",
    "        self.location_std = {}\n",
    "        self.scaler = {}\n",
    "\n",
    "        self._load_time_series()\n",
    "\n",
    "        print(f\"Loaded {len(self.sequence_data)} locations with {len(self.valid_samples)} valid samples.\")\n",
    "\n",
    "    def _load_entity_ids(self, path_ids: str) -> List[str]:\n",
    "        \"\"\"Opens simple text file and reads each line as a location id & returns a list of ids\"\"\"\n",
    "        if not os.path.isfile(path_ids):\n",
    "            raise FileNotFoundError(f\"ID file not found: {path_ids}\")\n",
    "        with open(path_ids, \"r\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    def _load_static_attributes(self, path: str) -> pd.DataFrame:\n",
    "        \"\"\"Reads a csv file with static attributes, sets location id as index, checks which ids from *entities_ids*\n",
    "        exist, and skips locations with missing attributes with warning - returns filtered df with valid ids\"\"\"\n",
    "        df = pd.read_csv(path).set_index('location_id')\n",
    "        available_ids = set(df.index)\n",
    "        valid_ids = [loc for loc in self.entities_ids if loc in available_ids]\n",
    "        missing = set(self.entities_ids) - available_ids\n",
    "        if missing:\n",
    "            print(f\"Warning: Skipping {len(missing)} locations with missing static attributes.\")\n",
    "        self.entities_ids = valid_ids\n",
    "        return df.loc[self.entities_ids]\n",
    "\n",
    "    def _load_time_series(self):\n",
    "        \"\"\"Builds file path, loads the csvs (must have date column); checks if all dynamic and target columns are available;\n",
    "        extracts different features; returns a mask saying which time indeices can be used to form valid sequences (NaN filter);\n",
    "        stores everything in PyTorch tensors\"\"\"\n",
    "        for loc_id in self.entities_ids:\n",
    "            ts_path = os.path.join(self.path_data, f\"{loc_id}.csv\")\n",
    "            if not os.path.isfile(ts_path):\n",
    "                print(f\"Warning: Missing file {ts_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df_ts = pd.read_csv(ts_path, parse_dates=['date'])\n",
    "            if not all(col in df_ts.columns for col in self.dynamic_input + self.target):\n",
    "                print(f\"Warning: {loc_id} missing required columns. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            x_dynamic = df_ts[self.dynamic_input].values\n",
    "            y_target = df_ts[self.target].values\n",
    "            dates = df_ts['date'].values\n",
    "\n",
    "            attributes = self.df_attributes.loc[loc_id, self.static_input].values if self.static_input else None\n",
    "\n",
    "            flags = validate_samples(x_dynamic, y_target, attributes, self.sequence_length, self.check_NaN)\n",
    "\n",
    "            for idx in np.flatnonzero(flags == 1):\n",
    "                self.valid_samples.append((loc_id, idx))\n",
    "\n",
    "            self.sequence_data[loc_id] = {\n",
    "                'x_d': torch.tensor(x_dynamic, dtype=torch.float32),\n",
    "                'y': torch.tensor(y_target, dtype=torch.float32),\n",
    "                'dates': dates\n",
    "            }\n",
    "            if self.static_input:\n",
    "                self.sequence_data[loc_id]['x_s'] = torch.tensor(attributes, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of valid training samples\"\"\"\n",
    "        return len(self.valid_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"For a given index in the dataset finds the corresponding location and time index and extracts\n",
    "        the dynamic input sequence; repeat static features over the sequence and concatenate them with\n",
    "        corresponding dynamic features\"\"\"\n",
    "        location, i = self.valid_samples[idx]\n",
    "        data = self.sequence_data[location]\n",
    "\n",
    "        x_dynamic_seq = data['x_d'][i - self.sequence_length + 1:i + 1]\n",
    "        \n",
    "        if self.static_input:\n",
    "            x_static = data['x_s'].repeat(self.sequence_length, 1)\n",
    "            x_input = torch.cat([x_dynamic_seq, x_static], dim=1)\n",
    "        else:\n",
    "            x_input = x_dynamic_seq\n",
    "\n",
    "        y_target = data['y'][i]\n",
    "\n",
    "        if self.location_std:\n",
    "            return x_input, y_target, self.location_std[location].unsqueeze(0)\n",
    "        return x_input, y_target\n",
    "\n",
    "    def calculate_location_std(self):\n",
    "        \"\"\"Compute per-location standard deviation of target variable.\"\"\"\n",
    "        for loc, data in self.sequence_data.items():\n",
    "            self.location_std[loc] = data['y'].nanstd().to(dtype=torch.float32)\n",
    "\n",
    "    def calculate_global_statistics(self):\n",
    "        \"\"\"Compute global mean/std using NumPy to handle NaNs.\"\"\"\n",
    "        if not self.sequence_data:\n",
    "            raise ValueError(\"No time series data available.\")\n",
    "\n",
    "        # Convert to NumPy for nan-safe stats\n",
    "        all_x = torch.cat([d['x_d'] for d in self.sequence_data.values()], dim=0).numpy()\n",
    "        all_y = torch.cat([d['y'] for d in self.sequence_data.values()], dim=0).numpy()\n",
    "\n",
    "        self.scaler = {\n",
    "            'x_d_mean': torch.tensor(np.nanmean(all_x, axis=0), dtype=torch.float32),\n",
    "            'x_d_std': torch.tensor(np.nanstd(all_x, axis=0), dtype=torch.float32),\n",
    "            'y_mean': torch.tensor(np.nanmean(all_y, axis=0), dtype=torch.float32),\n",
    "            'y_std': torch.tensor(np.nanstd(all_y, axis=0), dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "        if self.static_input:\n",
    "            attr_np = self.df_attributes[self.static_input].values\n",
    "            self.scaler['x_s_mean'] = torch.tensor(np.nanmean(attr_np, axis=0), dtype=torch.float32)\n",
    "            self.scaler['x_s_std'] = torch.tensor(np.nanstd(attr_np, axis=0), dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def standardize_data(self, standardize_output: bool = True):\n",
    "        \"\"\"Standardize inputs and optionally outputs using precomputed stats.\"\"\"\n",
    "        eps = 1e-6 # Avoids division by zero\n",
    "        for data in self.sequence_data.values():\n",
    "            data['x_d'] = (data['x_d'] - self.scaler['x_d_mean']) / (self.scaler['x_d_std'] + eps)\n",
    "            \n",
    "            if self.static_input:\n",
    "                data['x_s'] = (data['x_s'] - self.scaler['x_s_mean']) / (self.scaler['x_s_std'] + eps)\n",
    "            \n",
    "            if standardize_output:\n",
    "                data['y'] = (data['y'] - self.scaler['y_mean']) / (self.scaler['y_std'] + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cad5c-01e4-454c-b4b0-cc472e28c7a8",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be73d4b-dd84-4bee-b529-6e0117b48bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(running_device: str = \"gpu\") -> torch.device:\n",
    "    \"\"\"Checks if user requested gpu and if CUDA is available\"\"\"\n",
    "    if running_device == \"gpu\" and torch.cuda.is_available():\n",
    "        print(f\"✅ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        print(\"⚠️ Using CPU (GPU not available or overridden)\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class MDN_LSTM(nn.Module):\n",
    "    def __init__(self, model_hyper_parameters, num_mixtures=3, temperature: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.input_size = model_hyper_parameters['input_size']\n",
    "        self.hidden_size = model_hyper_parameters['hidden_size']\n",
    "        self.num_layers = model_hyper_parameters['no_of_layers']\n",
    "        self.dropout_rate = model_hyper_parameters['drop_out']\n",
    "        self.num_mixtures = num_mixtures # Number of Gaussan components in the output (default=3)\n",
    "        self.temperature = temperature # Controls how \"soft\" or \"peaked\" the mixture weights are\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        self.fc_pi = nn.Linear(self.hidden_size, self.num_mixtures) # Output raw logits for mixture weights\n",
    "        self.fc_mu = nn.Linear(self.hidden_size, self.num_mixtures) # Output means of each Gaussian\n",
    "        self.fc_sigma = nn.Linear(self.hidden_size, self.num_mixtures) # Outputs raw values that will be transformed to positive standard deviation\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param) # Xavier initialization - good for stable gradients\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "        for layer in [self.fc_pi, self.fc_mu, self.fc_sigma]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn_like(x) * 0.005 # Adds small Gaussian noise to inputs during training - helps regularize the model and avoid overfitting\n",
    "        x = x + noise\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])  # take last time step\n",
    "\n",
    "        return self._mdn_head(out)\n",
    "\n",
    "    def _mdn_head(self, lstm_output):\n",
    "        # Apply temperature scaling to logits before softmax - dividing by temperature affects the softness of softmax\n",
    "        # Temperature < 1 - makes the distribution sharper\n",
    "        # Temperature > 1 - makes the distribution softer (more uniform)\n",
    "        logits = self.fc_pi(lstm_output)\n",
    "        pi = F.softmax(logits / self.temperature, dim=-1)\n",
    "\n",
    "        mu = self.fc_mu(lstm_output) # Gives the mean of each Gaussian component\n",
    "        sigma = F.softplus(self.fc_sigma(lstm_output)) + 1e-6  # ensure positive\n",
    "\n",
    "        return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8593170-b0b3-4111-a9d0-419334d538f7",
   "metadata": {},
   "source": [
    "Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda587d9-31aa-4885-ad3c-b530ab4f2f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Loaded 2212 locations with 4752554 valid samples.\n",
      "Loaded 545 locations with 1129435 valid samples.\n",
      "✅ Using GPU: NVIDIA A100-SXM4-40GB\n",
      "Fold 1 | Epoch 01 | Loss: 0.3268 | LR: 0.00050 | Time: 450.3s\n",
      "Fold 1 | Epoch 02 | Loss: -0.0423 | LR: 0.00025 | Time: 454.3s\n",
      "Fold 1 | Epoch 03 | Loss: -0.1775 | LR: 0.00013 | Time: 459.4s\n",
      "Fold 1 | Epoch 04 | Loss: -0.2555 | LR: 0.00006 | Time: 454.5s\n",
      "Fold 1 | Epoch 05 | Loss: -0.2997 | LR: 0.00003 | Time: 449.4s\n",
      "Fold 1 | Epoch 06 | Loss: -0.3243 | LR: 0.00002 | Time: 462.2s\n",
      "Fold 1 | Epoch 07 | Loss: -0.3378 | LR: 0.00001 | Time: 464.2s\n",
      "Fold 1 | Epoch 08 | Loss: -0.3441 | LR: 0.00000 | Time: 460.9s\n",
      "Fold 1 | Epoch 09 | Loss: -0.3477 | LR: 0.00000 | Time: 455.4s\n",
      "Fold 1 | Epoch 10 | Loss: -0.3496 | LR: 0.00000 | Time: 451.8s\n",
      "✅ Fold 1 training done in 4562.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 0] Processing test locations:   0%|          | 0/545 [00:00<?, ?it/s]/software/all/jupyter/ai/2025-05-23/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "[Fold 0] Processing test locations: 100%|██████████| 545/545 [00:58<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fold 0 results to: results_daymet_final/fold_1/results/total_locations_fold_0.csv\n",
      "✅ Saved 545 individual filtered files to: results_daymet_final/fold_1/results/filtered_per_location\n",
      "\n",
      "=== Fold 2 ===\n",
      "Loaded 2207 locations with 4599266 valid samples.\n",
      "Loaded 550 locations with 1282723 valid samples.\n",
      "✅ Using GPU: NVIDIA A100-SXM4-40GB\n",
      "Fold 2 | Epoch 01 | Loss: 0.3304 | LR: 0.00050 | Time: 447.3s\n",
      "Fold 2 | Epoch 02 | Loss: -0.0261 | LR: 0.00025 | Time: 445.9s\n",
      "Fold 2 | Epoch 03 | Loss: -0.1545 | LR: 0.00013 | Time: 446.6s\n",
      "Fold 2 | Epoch 04 | Loss: -0.2287 | LR: 0.00006 | Time: 448.9s\n",
      "Fold 2 | Epoch 05 | Loss: -0.2713 | LR: 0.00003 | Time: 446.6s\n",
      "Fold 2 | Epoch 06 | Loss: -0.2947 | LR: 0.00002 | Time: 443.6s\n",
      "Fold 2 | Epoch 07 | Loss: -0.3070 | LR: 0.00001 | Time: 443.0s\n",
      "Fold 2 | Epoch 08 | Loss: -0.3137 | LR: 0.00000 | Time: 443.0s\n",
      "Fold 2 | Epoch 09 | Loss: -0.3170 | LR: 0.00000 | Time: 442.9s\n",
      "Fold 2 | Epoch 10 | Loss: -0.3188 | LR: 0.00000 | Time: 442.8s\n",
      "✅ Fold 2 training done in 4450.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1] Processing test locations: 100%|██████████| 550/550 [01:05<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fold 1 results to: results_daymet_final/fold_2/results/total_locations_fold_1.csv\n",
      "✅ Saved 550 individual filtered files to: results_daymet_final/fold_2/results/filtered_per_location\n",
      "\n",
      "=== Fold 3 ===\n",
      "Loaded 2179 locations with 4708201 valid samples.\n",
      "Loaded 578 locations with 1173788 valid samples.\n",
      "✅ Using GPU: NVIDIA A100-SXM4-40GB\n",
      "Fold 3 | Epoch 01 | Loss: 0.3109 | LR: 0.00050 | Time: 454.5s\n",
      "Fold 3 | Epoch 02 | Loss: -0.0623 | LR: 0.00025 | Time: 457.4s\n",
      "Fold 3 | Epoch 03 | Loss: -0.1953 | LR: 0.00013 | Time: 457.3s\n",
      "Fold 3 | Epoch 04 | Loss: -0.2712 | LR: 0.00006 | Time: 457.4s\n",
      "Fold 3 | Epoch 05 | Loss: -0.3147 | LR: 0.00003 | Time: 457.8s\n",
      "Fold 3 | Epoch 06 | Loss: -0.3389 | LR: 0.00002 | Time: 458.6s\n",
      "Fold 3 | Epoch 07 | Loss: -0.3515 | LR: 0.00001 | Time: 458.0s\n",
      "Fold 3 | Epoch 08 | Loss: -0.3578 | LR: 0.00000 | Time: 457.5s\n",
      "Fold 3 | Epoch 09 | Loss: -0.3615 | LR: 0.00000 | Time: 457.6s\n",
      "Fold 3 | Epoch 10 | Loss: -0.3628 | LR: 0.00000 | Time: 457.6s\n",
      "✅ Fold 3 training done in 4573.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2] Processing test locations:  17%|█▋        | 99/578 [00:10<00:54,  8.76it/s]"
     ]
    }
   ],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"mkdir if missing\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def to_numpy_1d(tensor):\n",
    "    \"\"\"detach-cpu-numpy-flatten (for saving to csv)\"\"\"\n",
    "    return tensor.detach().cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "\"\"\"unscale/unscale_feature/unscale_scalar - reverse standardization using stored means/stds\"\"\"\n",
    "\n",
    "def unscale(value, mean, std):\n",
    "    return value * std + mean\n",
    "\n",
    "def unscale_feature(tensor, idx, scaler_mean, scaler_std):\n",
    "    return unscale(to_numpy_1d(tensor[:, idx]), scaler_mean[idx].item(), scaler_std[idx].item())\n",
    "\n",
    "def unscale_scalar(value, idx, scaler_mean, scaler_std):\n",
    "    return unscale(value, scaler_mean[idx].item(), scaler_std[idx].item())\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    \"\"\"Sets seeds for random, numpy, torch, torch.cuda and configures cuDNN for detetrministic execution\n",
    "    - enables reproducible folds/runs\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call this once before everything\n",
    "seed = 42\n",
    "set_random_seed(seed)\n",
    "\n",
    "# Group entity IDs by location\n",
    "# Entity ids look like *location_depth* - grouping entities under a base location name before cross-validation\n",
    "# Then perform 5-fold cross-validation on locations(not on individual series) to avoid spatial leakage\n",
    "location_to_entities = defaultdict(list)\n",
    "for eid in all_entity_ids:\n",
    "    location = eid.split('_')[0]  # e.g., 'Abrams_0.05' → 'Abrams'\n",
    "    location_to_entities[location].append(eid)\n",
    "\n",
    "all_locations = list(location_to_entities.keys())\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 5-fold cross-validation loop\n",
    "for fold, (train_loc_idx, val_loc_idx) in enumerate(kf.split(all_locations)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "\n",
    "    train_locations = [all_locations[i] for i in train_loc_idx]\n",
    "    val_locations = [all_locations[i] for i in val_loc_idx]\n",
    "\n",
    "    train_ids = [eid for loc in train_locations for eid in location_to_entities[loc]]\n",
    "    val_ids = [eid for loc in val_locations for eid in location_to_entities[loc]]\n",
    "\n",
    "    path_save_folder_fold = os.path.join(path_save_folder, f\"fold_{fold+1}\")\n",
    "    create_folder(path_save_folder_fold)\n",
    "\n",
    "    # Create datasets - Train dataset loads all training entities, computes global mean/stds from\n",
    "    # train only, and standardizes x_d, x_s, y\n",
    "    training_dataset = BaseDataset(\n",
    "        dynamic_input=dynamic_input,\n",
    "        static_input=static_input,\n",
    "        target=target,\n",
    "        sequence_length=model_hyper_parameters['seq_length'],\n",
    "        path_entities_ids=train_ids,\n",
    "        path_data=path_data,\n",
    "        path_static_attributes=path_static,\n",
    "        set_type='train',\n",
    "        check_NaN=True\n",
    "    )\n",
    "    training_dataset.calculate_global_statistics()\n",
    "    training_dataset.standardize_data()\n",
    "\n",
    "    # Validation (or test) dataset uses train scalers, but does not standardize y\n",
    "    # (so ground truth stays in original scale for easier reporting/metrics)\n",
    "    validation_dataset = BaseDataset(\n",
    "        dynamic_input=dynamic_input,\n",
    "        static_input=static_input,\n",
    "        target=target,\n",
    "        sequence_length=model_hyper_parameters['seq_length'],\n",
    "        path_entities_ids=val_ids,\n",
    "        path_data=path_data,\n",
    "        path_static_attributes=path_static,\n",
    "        set_type='val',\n",
    "        check_NaN=True\n",
    "    )\n",
    "    validation_dataset.scaler = training_dataset.scaler\n",
    "    validation_dataset.standardize_data(standardize_output=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=model_hyper_parameters['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # For validation creates one batch per location - the batch sampler uses indices so each batch\n",
    "    # equals all valid timesteps for one location\n",
    "    validation_batches = [\n",
    "        [i for i, _ in group]\n",
    "        for _, group in groupby(\n",
    "            enumerate(validation_dataset.valid_samples),\n",
    "            key=lambda x: x[1][0]\n",
    "        )\n",
    "    ]\n",
    "    validation_loader = DataLoader(\n",
    "        dataset=validation_dataset,\n",
    "        batch_sampler=validation_batches\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    set_random_seed(seed=seed)\n",
    "    device = get_device(\"gpu\")\n",
    "    model = MDN_LSTM(model_hyper_parameters).to(device)\n",
    "\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_hyper_parameters[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=model_hyper_parameters[\"adapt_learning_rate_epoch\"], gamma=model_hyper_parameters[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "    model.lstm.bias_hh_l0.data[model_hyper_parameters['hidden_size']: 2 * model_hyper_parameters['hidden_size']] = model_hyper_parameters[\"set_forget_gate\"]\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, model_hyper_parameters[\"no_of_epochs\"] + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for x_lstm, y in train_loader:\n",
    "            x_lstm = x_lstm.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type='cuda'):\n",
    "                pi, mu, sigma = model(x_lstm)\n",
    "                loss = nll_loss(pi, mu, sigma, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        report = f\"Fold {fold+1} | Epoch {epoch:02d} | Loss: {avg_loss:.4f} | LR: {lr:.5f} | Time: {epoch_time:.1f}s\"\n",
    "        print(report)\n",
    "        write_report(f'{path_save_folder_fold}/run_progress.txt', report)\n",
    "\n",
    "        torch.save(model.state_dict(), f'{path_save_folder_fold}/epoch_{epoch}.pt')\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(f\"Fold {fold+1} training done in {time.time() - training_start_time:.1f}s\")\n",
    "\n",
    "    # Evaluate and save results\n",
    "    test_dataset = validation_dataset\n",
    "    test_loader = validation_loader\n",
    "\n",
    "    valid_location_testing = [loc_id for loc_id, _ in groupby(test_dataset.valid_samples, key=lambda x: x[0])]\n",
    "    valid_entity_per_location_testing = [\n",
    "        [i for _, i in group]\n",
    "        for _, group in groupby(test_dataset.valid_samples, key=lambda x: x[0])\n",
    "    ]\n",
    "\n",
    "    results_dir = os.path.join(path_save_folder_fold, 'results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    pi_list, mu_list, sigma_list = [], [], []\n",
    "    test_results = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x_lstm, y) in enumerate(tqdm(test_loader, desc=f\"[Fold {fold}] Processing test locations\")):\n",
    "            x_lstm = x_lstm.to(device).float()\n",
    "            pi, mu, sigma = model(x_lstm)\n",
    "\n",
    "            y_std = test_dataset.scaler['y_std'].to(device)\n",
    "            y_mean = test_dataset.scaler['y_mean'].to(device)\n",
    "            mu = mu * y_std + y_mean\n",
    "            sigma = sigma * y_std\n",
    "\n",
    "            location_id = valid_location_testing[i]\n",
    "            seq_data = test_dataset.sequence_data[location_id]\n",
    "            indices = valid_entity_per_location_testing[i]\n",
    "            dates = seq_data['dates'][indices]\n",
    "\n",
    "            df_data = {'date': dates}\n",
    "            df_data['soil_moisture'] = to_numpy_1d(seq_data['y'][indices])\n",
    "            x_d = seq_data['x_d']\n",
    "            x_s = seq_data['x_s']\n",
    "\n",
    "            # If features exist add to output (optional)\n",
    "           #for feature in ['']:\n",
    "               # if feature in test_dataset.dynamic_input:\n",
    "                #    idx = test_dataset.dynamic_input.index(feature)\n",
    "                 #   df_data[feature] = unscale_feature(x_d[indices], idx,\n",
    "                                                       test_dataset.scaler['x_d_mean'],\n",
    "                                                       test_dataset.scaler['x_d_std'])\n",
    "\n",
    "           \n",
    "            # Add lon/lat columns to output - useful for evaluation, although can be done later as well\n",
    "            for static_feature in ['lon', 'lat']:\n",
    "                if static_feature in test_dataset.static_input:\n",
    "                    idx = test_dataset.static_input.index(static_feature)\n",
    "                    value = unscale_scalar(x_s[idx], idx,\n",
    "                                           test_dataset.scaler['x_s_mean'],\n",
    "                                           test_dataset.scaler['x_s_std'])\n",
    "                    df_data[static_feature] = np.full(len(dates), value)\n",
    "\n",
    "            for k in range(pi.shape[1]): # Add MDN params\n",
    "                df_data[f'pi_{k+1}'] = to_numpy_1d(pi[:, k])\n",
    "                df_data[f'mu_{k+1}'] = to_numpy_1d(mu[:, k])\n",
    "                df_data[f'sigma_{k+1}'] = to_numpy_1d(sigma[:, k])\n",
    "\n",
    "            test_results.append(pd.DataFrame(df_data))\n",
    "            pi_list.append(pi.cpu().numpy())\n",
    "            mu_list.append(mu.cpu().numpy())\n",
    "            sigma_list.append(sigma.cpu().numpy())\n",
    "\n",
    "        # Save concatenated csvs across locations\n",
    "        all_columns = set().union(*(df.columns for df in test_results))\n",
    "        desired_columns = [\n",
    "            'date', 'soil_moisture',\n",
    "            'lon', 'lat', 'pi_1', 'pi_2', 'pi_3', 'sigma_1', 'sigma_2', 'sigma_3', 'mu_1', 'mu_2', 'mu_3'\n",
    "        ]\n",
    "        used_columns = [col for col in desired_columns if col in all_columns]\n",
    "\n",
    "        total_df = pd.concat([\n",
    "            df[used_columns] for df in test_results\n",
    "        ], ignore_index=True).dropna(subset=['date', 'soil_moisture']) \n",
    "\n",
    "    output_path = os.path.join(results_dir, f'total_locations_fold_{fold}.csv')\n",
    "    total_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved fold {fold} results to: {output_path}\")\n",
    "\n",
    "    # Save per-location fileterd csvs (for location-specific evaluation)\n",
    "    output_folder = os.path.join(results_dir, f\"filtered_per_location\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    desired_columns = [\n",
    "        'date', 'soil_moisture',\n",
    "        'lon', 'lat', 'pi_1', 'pi_2', 'pi_3', 'sigma_1', 'sigma_2', 'sigma_3', 'mu_1', 'mu_2', 'mu_3'\n",
    "    ]\n",
    "\n",
    "    test_results_dict = {\n",
    "        valid_location_testing[i]: result\n",
    "        for i, result in enumerate(test_results)\n",
    "    }\n",
    "\n",
    "    for location, df_ts in test_results_dict.items():\n",
    "        available_columns = [col for col in desired_columns if col in df_ts.columns]\n",
    "        filtered_df = df_ts[available_columns].copy()\n",
    "        if 'date' in filtered_df.columns:\n",
    "            filtered_df.sort_values('date', inplace=True)\n",
    "        filepath = os.path.join(output_folder, f\"{location}_filtered.csv\")\n",
    "        filtered_df.to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"Saved {len(test_results_dict)} individual filtered files to: {output_folder}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
